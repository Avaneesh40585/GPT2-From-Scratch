{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "042ebf93",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Inference Notebook\n",
    "\n",
    "Use this notebook to interact with your trained model. This notebook is designed to:\n",
    "1.  **Load Weights:** Import your trained model checkpoints & original model checkpoints (specifically `.pth` files).\n",
    "2.  **Generate Text:** Run inference to produce new text based on your custom training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8328a97e-bb87-4aca-84c7-4366e4427a63",
   "metadata": {},
   "source": [
    "## Imports & Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a30162-2634-490d-8bdc-fd8fdd6b421a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "# Import architecture and generation utilities\n",
    "from model import GPTModel\n",
    "from generate import generate_and_print\n",
    "from download_weights import download_and_save_gpt2\n",
    "\n",
    "# Device Setup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Inference Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a57f531-8ecf-4f76-b9bc-45f3c9070688",
   "metadata": {},
   "source": [
    "## Model Selection & Configuration\n",
    "\n",
    "**Choose Your Model:**\n",
    "In the cell below, you can toggle `MODEL_SELECTION` between your own trained model or one of the official pre-trained GPT-2 models from OpenAI.\n",
    "\n",
    "* **Custom Model (`\"custom\"`):** Loads the `.pth` file you trained earlier from the `./models` directory.\n",
    "    * *Important:* Ensure the configuration inside the `if` block (especially `context_length`) matches your training settings exactly.\n",
    "* **Official Models:** Downloads and converts the original OpenAI weights automatically.\n",
    "    * *Available Sizes:* `gpt2-small (124M)`, `gpt2-medium (355M)`, `gpt2-large (774M)`, `gpt2-xl (1558M)`.\n",
    "    * *Hardware Warning:* The **XL (1558M)** model requires significant RAM (~12GB+) during the initial conversion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81d83d67-f9c5-4188-ba72-4d6fe3761730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selection: Official OpenAI GPT-2 (gpt2-small (124M))\n",
      "Downloading 124M files...\n",
      "File already exists: ./model_weights/tf_weights/124M/checkpoint\n",
      "File already exists: ./model_weights/tf_weights/124M/encoder.json\n",
      "File already exists: ./model_weights/tf_weights/124M/hparams.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.ckpt.data-00000-of-00001: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 498M/498M [19:45<00:00, 420kiB/s]\n",
      "model.ckpt.index: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 5.21k/5.21k [00:00<00:00, 5.36MiB/s]\n",
      "model.ckpt.meta: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 471k/471k [00:01<00:00, 277kiB/s]\n",
      "vocab.bpe: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:01<00:00, 345kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TensorFlow weights...\n",
      "Converting to PyTorch...\n",
      "Saving PyTorch model to ./model_weights/gpt2_124M.pth...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Available configurations for gpt2\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\":  {\"emb_dim\": 768,  \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\":  {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\":    {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# --- USER CHOICE ---\n",
    "# You can copy-paste keys from above:\n",
    "# \"gpt2-small (124M)\", \"gpt2-medium (355M)\", \"gpt2-large (774M)\", \"gpt2-xl (1558M)\"\n",
    "# OR set to \"custom\" for your locally trained model.\n",
    "MODEL_SELECTION = \"gpt2-small (124M)\" \n",
    "\n",
    "\n",
    "# Directory to store models\n",
    "MODELS_DIR = \"./model_weights\"\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "if MODEL_SELECTION == \"custom\":\n",
    "    # --- PATH A: YOUR CUSTOM MODEL ---\n",
    "    # Ensure this filename matches exactly what you saved in train.ipynb!\n",
    "    filename = \"trained_gpt2.pth\" \n",
    "    model_path = os.path.join(MODELS_DIR, filename)\n",
    "    print(f\"\\nSelection: Custom Model ({model_path})\")\n",
    "    \n",
    "    config = {\n",
    "        \"vocab_size\": 50257,    \n",
    "        \"context_length\": 256, \n",
    "        \"emb_dim\": 768,         \n",
    "        \"n_heads\": 12,          \n",
    "        \"n_layers\": 12,         \n",
    "        \"drop_rate\": 0.1,       \n",
    "        \"qkv_bias\": False       \n",
    "    }\n",
    "\n",
    "elif MODEL_SELECTION in model_configs:\n",
    "    # --- PATH B: OFFICIAL OPENAI MODEL ---\n",
    "    print(f\"\\nSelection: Official OpenAI GPT-2 ({MODEL_SELECTION})\")\n",
    "    \n",
    "    # 1. Parse the size ID from the string (e.g., extract \"124M\" from \"gpt2-small (124M)\")\n",
    "    # We split by '(' and take the second part, then remove the closing ')'\n",
    "    size_id = MODEL_SELECTION.split(\"(\")[1].split(\")\")[0]\n",
    "    \n",
    "    # 2. Download & Load\n",
    "    # Pass the clean ID (\"124M\") to the downloader\n",
    "    model_path = download_and_save_gpt2(size_id, MODELS_DIR, GPTModel)\n",
    "    \n",
    "    # 3. Apply Configuration\n",
    "    config = {\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 1024,\n",
    "        \"drop_rate\": 0.0,\n",
    "        \"qkv_bias\": True # OpenAI Official Weights always have Bias=True\n",
    "    }\n",
    "    # Update with specific dimensions (emb_dim, layers, heads)\n",
    "    config.update(model_configs[MODEL_SELECTION])\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model selection: {MODEL_SELECTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5fc266-0a3a-4c2d-8abc-0d494f10d740",
   "metadata": {},
   "source": [
    "## Loading Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ae4ec39-d578-44e9-87d4-fb7dd54c9ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ./model_weights/gpt2_124M.pth ...\n",
      "-> Model successfully loaded and ready!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading model from: {model_path} ...\")\n",
    "\n",
    "model = GPTModel(config)\n",
    "state_dict = torch.load(model_path, map_location=device, weights_only=True)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"-> Model successfully loaded and ready!\")\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451a713a-7926-4dec-b93c-b2ef8f00e380",
   "metadata": {},
   "source": [
    "## Interactive Generation\n",
    "\n",
    "Run the cell below to generate text. \n",
    "\n",
    "* **`start_context`**: The prompt you give the model.\n",
    "* **`max_new_tokens`**: How much text to write.\n",
    "* **`temperature`**: Controls creativity.\n",
    "    * `0.0` = Deterministic (Always picks the most likely word).\n",
    "    * `0.8` = Creative (More random, diverse).\n",
    "    * `1.2` = Chaotic (Can be incoherent).\n",
    "* **`top_k`**: Limits the vocabulary choices to the $K$ most likely next tokens. This prevents the model from picking highly improbable \"junk\" words that can ruin the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59adebdd-d580-48a9-916d-6c0f150fa390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Gen]: The history of artificial intelligence is not without its challenges. It is also a history of ignorance. We must learn from the mistakes of others and work to overcome them.  It is a history of ignorance that we have not learned from. We have learned to accept the limitations of our own thinking, to seek outside the box, to have patience and to seek out alternatives.  We have learned to accept that there may be some things that we cannot control, that we may not completely control, that we may not fully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup Tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# --- USER SETTINGS ---\n",
    "PROMPT = \"The history of artificial intelligence\"\n",
    "LENGTH = 100\n",
    "TEMP   = 0.7  # Try 0.5 to 1.0 for best results\n",
    "TOP_K  = 50   # Restrict to top 50 probable words (prevents weird artifacts)\n",
    "\n",
    "# Run Generation\n",
    "print(f\"Generating for prompt: '{PROMPT}'\\n\" + \"-\"*50)\n",
    "output = generate_and_print(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    device, \n",
    "    start_context=PROMPT, \n",
    "    max_new_tokens=LENGTH, \n",
    "    temperature=TEMP,\n",
    "    top_k=TOP_K\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
