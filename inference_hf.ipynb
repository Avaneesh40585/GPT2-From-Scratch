{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9d20148-52c9-4ef0-a9ab-5cc8fd085eac",
   "metadata": {},
   "source": [
    "# Inference Notebook\n",
    "\n",
    "Use this notebook to interact with your trained model. This notebook is designed to:\n",
    "1.  **Load Weights:** Import your trained model checkpoints & original model checkpoints (specifically `.pth` files).\n",
    "2.  **Generate Text:** Run inference to produce new text based on your custom training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7f98f9-1719-4a76-85e8-f5b1f5c82419",
   "metadata": {},
   "source": [
    "## Imports & Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5122a444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "from model import GPTModel\n",
    "from generate import generate_and_print\n",
    "from download_weights_hf import load_weights_into_gpt \n",
    "\n",
    "# Import Transformers for downloading official weights\n",
    "from transformers import GPT2Model\n",
    "\n",
    "# Device Setup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Inference Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9e5a3e-2607-4f33-aba0-66ce9767415f",
   "metadata": {},
   "source": [
    "## Model Selection & Configuration\n",
    "\n",
    "**Choose Your Model:**\n",
    "In the cell below, you can toggle `MODEL_SELECTION` between your own trained model or one of the official pre-trained GPT-2 models (loaded via Hugging Face).\n",
    "\n",
    "* **Custom Model (`\"custom\"`):** Loads the `.pth` file you trained earlier from the `./model_weights` directory.\n",
    "    * *Important:* Ensure the configuration inside the `if` block (especially `context_length`) matches your training settings exactly.\n",
    "* **Official Models:** Downloads pre-trained weights from the Hugging Face Hub and maps them into our custom architecture on the fly.\n",
    "    * *Available Sizes:* `gpt2-small (124M)`, `gpt2-medium (355M)`, `gpt2-large (774M)`, `gpt2-xl (1558M)`.\n",
    "    * *Mechanism:* This uses the `transformers` library to fetch weights, then transfers them layer-by-layer into your `GPTModel` class.\n",
    "    * *Hardware Warning:* The **XL (1558M)** model requires significant RAM (~12GB+) during the loading process because it must hold both the source weights and your custom model in memory simultaneously before cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1656eaa7-03c3-4d72-9194-fbbfc9a1ec57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selection: Official OpenAI GPT-2 (gpt2-small (124M))\n",
      "Configuration set.\n"
     ]
    }
   ],
   "source": [
    "# Available configurations for gpt2\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\":  {\"emb_dim\": 768,  \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\":  {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\":    {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# --- USER CHOICE ---\n",
    "# You can copy-paste keys from above:\n",
    "# \"gpt2-small (124M)\", \"gpt2-medium (355M)\", \"gpt2-large (774M)\", \"gpt2-xl (1558M)\"\n",
    "# OR set to \"custom\" for your locally trained model.\n",
    "MODEL_SELECTION = \"gpt2-small (124M)\" \n",
    "\n",
    "# Directory to store models\n",
    "MODELS_DIR = \"./model_weights\"\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# 1. Setup Configuration based on selection\n",
    "if MODEL_SELECTION == \"custom\":\n",
    "    # --- PATH A: YOUR CUSTOM MODEL ---\n",
    "    filename = \"trained_gpt2.pth\" \n",
    "    model_path = os.path.join(MODELS_DIR, filename)\n",
    "    print(f\"\\nSelection: Custom Model ({model_path})\")\n",
    "    \n",
    "    # Ensure this config matches your training exactly!\n",
    "    config = {\n",
    "        \"vocab_size\": 50257,    \n",
    "        \"context_length\": 256, \n",
    "        \"emb_dim\": 768,         \n",
    "        \"n_heads\": 12,          \n",
    "        \"n_layers\": 12,         \n",
    "        \"drop_rate\": 0.1,       \n",
    "        \"qkv_bias\": False       \n",
    "    }\n",
    "\n",
    "elif MODEL_SELECTION in model_configs:\n",
    "    # --- PATH B: OFFICIAL OPENAI MODEL (VIA HF) ---\n",
    "    print(f\"\\nSelection: Official OpenAI GPT-2 ({MODEL_SELECTION})\")\n",
    "    \n",
    "    # Official models always use context 1024 and Bias=True\n",
    "    config = {\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 1024,\n",
    "        \"drop_rate\": 0.0,\n",
    "        \"qkv_bias\": True \n",
    "    }\n",
    "    config.update(model_configs[MODEL_SELECTION])\n",
    "    model_path = None # No local file path needed yet\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model selection: {MODEL_SELECTION}\")\n",
    "\n",
    "print(\"Configuration set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da0d851-0f38-4243-9e24-c7fb37a8d074",
   "metadata": {},
   "source": [
    "## Instantiation & Weight Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4529340-3a18-44ea-a328-638cd9b9dde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Custom GPTModel...\n",
      "Downloading/Loading Official Weights via Hugging Face...\n",
      "Transferring weights to custom architecture...\n",
      "-> Weights successfully transferred from HF to Custom Model.\n",
      "-> Model successfully loaded and ready!\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize YOUR custom architecture\n",
    "print(f\"Initializing Custom GPTModel...\")\n",
    "model = GPTModel(config)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 2. Load Weights (Path A vs Path B)\n",
    "if MODEL_SELECTION == \"custom\":\n",
    "    print(f\"Loading local weights from: {model_path} ...\")\n",
    "    state_dict = torch.load(model_path, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "else:\n",
    "    print(f\"Downloading/Loading Official Weights via Hugging Face...\")\n",
    "    \n",
    "    # Map selection to HF ID\n",
    "    hf_mapping = {\n",
    "        \"gpt2-small (124M)\": \"gpt2\",\n",
    "        \"gpt2-medium (355M)\": \"gpt2-medium\",\n",
    "        \"gpt2-large (774M)\": \"gpt2-large\",\n",
    "        \"gpt2-xl (1558M)\": \"gpt2-xl\"\n",
    "    }\n",
    "    \n",
    "    # Download the HF model (to CPU first to save GPU RAM)\n",
    "    hf_model = GPT2Model.from_pretrained(\n",
    "        hf_mapping[MODEL_SELECTION], \n",
    "        cache_dir=MODELS_DIR\n",
    "    )\n",
    "    \n",
    "    # Transfer weights to custom model\n",
    "    print(\"Transferring weights to custom architecture...\")\n",
    "    load_weights_into_gpt(model, hf_model)\n",
    "    \n",
    "    # Re-assert the device move to ensure MPS allocates the new weight storage\n",
    "    model.to(device)\n",
    "    \n",
    "    # Cleanup HF model\n",
    "    del hf_model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"-> Model successfully loaded and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176cdc17-9fd5-4a13-843c-b063de642935",
   "metadata": {},
   "source": [
    "## Interactive Generation\n",
    "\n",
    "Run the cell below to generate text. \n",
    "\n",
    "* **`start_context`**: The prompt you give the model.\n",
    "* **`max_new_tokens`**: How much text to write.\n",
    "* **`temperature`**: Controls creativity.\n",
    "    * `0.0` = Deterministic (Always picks the most likely word).\n",
    "    * `0.8` = Creative (More random, diverse).\n",
    "    * `1.2` = Chaotic (Can be incoherent).\n",
    "* **`top_k`**: Limits the vocabulary choices to the $K$ most likely next tokens. This prevents the model from picking highly improbable \"junk\" words that can ruin the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca5c8ab2-4488-44b8-9f6f-907759d1cdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for prompt: 'The history of artificial intelligence'\n",
      "--------------------------------------------------\n",
      "\n",
      "[Gen]: The history of artificial intelligence in AI has been a fascinating one, ranging from the earliest days of AI to the present day. For example, the world of the human brain was first created in the 1960s by a group of researchers called the Institute for Advanced Study in the US. However, in the early 1980s, the team that was led by George Allen, a neuroscientist at the University of California, Berkeley, was led by David S. Siegel, an assistant professor of psychiatry at the University of California,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup Tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# --- USER SETTINGS ---\n",
    "PROMPT = \"The history of artificial intelligence\"\n",
    "LENGTH = 100\n",
    "TEMP   = 0.7  # Try 0.5 to 1.0 for best results\n",
    "TOP_K  = 50   # Restrict to top 50 probable words\n",
    "\n",
    "# Run Generation\n",
    "print(f\"Generating for prompt: '{PROMPT}'\\n\" + \"-\"*50)\n",
    "output = generate_and_print(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    device, \n",
    "    start_context=PROMPT, \n",
    "    max_new_tokens=LENGTH, \n",
    "    temperature=TEMP,\n",
    "    top_k=TOP_K\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
